{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import datetime\n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(x: np.ndarray, y: np.ndarray, label: list) -> None:\n",
    "    \"\"\"\n",
    "    Visualize it as an html\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): x\n",
    "        y (np.ndarray): y\n",
    "        label (list): label\n",
    "    \"\"\"\n",
    "    fig = px.line(\n",
    "        pd.DataFrame({'virtual_x': x, 'virtual_y': y}), x='virtual_x', y='virtual_y'\n",
    "    )\n",
    "    for l in label:\n",
    "        fig.add_vline(np.round(l[1], 8), line_dash='dash', annotation_text=l[0])\n",
    "    fig.update_layout(\n",
    "        annotations=[\n",
    "            {**a, **{\"y\": label[num][2]}}\n",
    "            for num, a in enumerate(fig.to_dict()[\"layout\"][\"annotations\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def get_next_point(\n",
    "    x_range: list,\n",
    "    init_value: int,\n",
    "    state: str,\n",
    "    max_length: int,\n",
    "    steady_angle=3,\n",
    "    beta_allowed_section=0.1,\n",
    "):\n",
    "\n",
    "    start, end = x_range\n",
    "    range_ = end - start\n",
    "\n",
    "    # valid_angle\n",
    "    steady_angle = np.min([steady_angle, np.arctan(1 - beta_allowed_section) / np.pi * 180])\n",
    "\n",
    "    all_x = np.linspace(1 / max_length, (max_length) / max_length, max_length)\n",
    "    allowed_lamp_angle = beta_allowed_section / (all_x * max_length) + np.tan(\n",
    "        np.pi / 180 * steady_angle\n",
    "    )\n",
    "\n",
    "    if state == 'steady':\n",
    "        allowed_range = np.tan(np.pi / 180 * steady_angle) * range_\n",
    "        upper_limit = np.min((init_value + allowed_range, 1))\n",
    "        lower_limit = np.max((init_value - allowed_range, 0))\n",
    "\n",
    "    else:\n",
    "        condition = np.where(np.abs(all_x - range_) <= 1e-8)\n",
    "        allowed_range = allowed_lamp_angle[condition[0][0]]\n",
    "        if 'up' in state:\n",
    "            upper_limit = 1\n",
    "            lower_limit = np.min((init_value + allowed_range, 1))\n",
    "        else:\n",
    "            upper_limit = np.max((init_value - allowed_range, 0))\n",
    "            lower_limit = 0\n",
    "\n",
    "        if lower_limit == upper_limit:\n",
    "            state = 'steady'\n",
    "\n",
    "    value = np.random.uniform(lower_limit, upper_limit)\n",
    "\n",
    "    return value, state\n",
    "\n",
    "\n",
    "def generate_x_y(\n",
    "    max_length: int,\n",
    "    split: int,\n",
    "    steady_angle: int = 3,\n",
    "    beta_allowed_section: float = 0.1,\n",
    "    steady_alpha: float = 2.0,\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    Create x and y values and choice first PR labels.\n",
    "    After, PR value and PR label are returned with the get_next_point function.\n",
    "\n",
    "    Args:\n",
    "        max_length (int): Max length of data\n",
    "        split (int): range value\n",
    "\n",
    "    Returns:\n",
    "        tuple: x and y and label applied PR logic\n",
    "    \"\"\"\n",
    "\n",
    "    all_point_x = np.linspace(1/max_length, (max_length - 2)/max_length, max_length - 2)\n",
    "    selected_x = np.random.choice(all_point_x, split - 1, replace=False)\n",
    "    sorted_selected_x = np.sort(selected_x)\n",
    "\n",
    "    x = np.concatenate(([0.0], sorted_selected_x, [(max_length - 1) / max_length]))\n",
    "    y = [np.random.uniform()]\n",
    "\n",
    "    state_list = ['up', 'down', 'steady']\n",
    "\n",
    "    state = []\n",
    "    real_state_list = []\n",
    "    for split_length in np.diff(x):\n",
    "        steady_prob = np.random.uniform()\n",
    "\n",
    "        # steady_criteria = split_length\n",
    "        steady_criteria = 1 / (1 + np.exp(-np.log(split_length / (1 / split)) * steady_alpha))\n",
    "\n",
    "        if steady_prob <= steady_criteria:\n",
    "            state.append('steady')\n",
    "        else:\n",
    "            state.append(np.random.choice([st for st in state_list if st != 'steady']))\n",
    "\n",
    "    for num, xx in enumerate(x[:-1]):\n",
    "        yy, real_state = get_next_point(\n",
    "            x[num : num + 2],\n",
    "            y[num],\n",
    "            state[num],\n",
    "            max_length,\n",
    "            steady_angle,\n",
    "            beta_allowed_section,\n",
    "        )\n",
    "        y.append(yy)\n",
    "        real_state_list.append(real_state)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y, real_state_list\n",
    "\n",
    "\n",
    "def get_label_from_state(x: np.ndarray, real_state: list) -> list:\n",
    "    \"\"\"\n",
    "    Grouped in same section.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): PR x values\n",
    "        real_state (list): PR labels\n",
    "\n",
    "    Returns:\n",
    "        list: Grouped PR labels\n",
    "    \"\"\"\n",
    "    init = real_state[0]\n",
    "    start = 0.0\n",
    "    label = []\n",
    "    for num, state in enumerate(real_state):\n",
    "        if state != init:\n",
    "            end = x[num]\n",
    "            label.append([init, start, end])\n",
    "            start = x[num]\n",
    "            init = state\n",
    "    label.append([init, start, 1.0])\n",
    "    return label\n",
    "\n",
    "\n",
    "def interpolate_x_y(x: np.ndarray, y: np.ndarray, max_length: int, noise_std: float) -> Tuple:\n",
    "    \"\"\"\n",
    "    Interpolate x and y\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): _description_\n",
    "        y (np.ndarray): _description_\n",
    "        max_length (int): Max length of data\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            'real' : Expressed detail of range more continuously\n",
    "    \"\"\"\n",
    "    x_real = np.linspace(0, (max_length - 1) / max_length, max_length)\n",
    "\n",
    "    y_real = []\n",
    "    for num, yy in enumerate(y[:-1]):\n",
    "        range_ = int(np.round((x[num + 1] - x[num]) * max_length, 0))\n",
    "        y_real.append(np.linspace(y[num], y[num + 1], range_ + 1)[:-1])\n",
    "    y_real.append(np.array([y[-1]]))\n",
    "    y_real = np.concatenate(y_real) + np.random.normal(0, noise_std, max_length)\n",
    "\n",
    "    # y_real = (y_real - np.min(y_real)) / (np.max(y_real) - np.min(y_real))\n",
    "\n",
    "    return x_real, y_real\n",
    "\n",
    "\n",
    "def generate_wrapper(\n",
    "    max_length: int,\n",
    "    split: int,\n",
    "    noise_std: float,\n",
    "    steady_angle: int = 3,\n",
    "    beta_allowed_section: float = 0.1,\n",
    "    steady_alpha: float = 2.0,\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate PR wrapper\n",
    "\n",
    "    Args:\n",
    "        max_length (int): Max length of data\n",
    "        split (int): range value\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            'real' : Expressed detail of range more continuously\n",
    "            label : label\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, real_state_list = generate_x_y(\n",
    "        max_length, split, steady_angle, beta_allowed_section, steady_alpha\n",
    "    )\n",
    "    label = get_label_from_state(x, real_state_list)\n",
    "\n",
    "    x_real, y_real = interpolate_x_y(x, y, max_length, noise_std)\n",
    "    return x_real, y_real, label\n",
    "\n",
    "\n",
    "def generate_sample(\n",
    "    max_length: int,\n",
    "    max_split: int,\n",
    "    sample_num: int,\n",
    "    noise_std: float = 0.001,\n",
    "    steady_angle: int = 3,\n",
    "    beta_allowed_section: float = 0.1,\n",
    "    steady_alpha: float = 2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate data and apply PR\n",
    "\n",
    "    Args:\n",
    "        max_length (int): Max length of data\n",
    "        max_split (int): Generate new elements of data from 3 to max_split\n",
    "        sample_num (int): Length of total data\n",
    "        noise_mean (int, optional): Defaults to 0.\n",
    "        noise_std (float, optional): Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        _type_: list\n",
    "\n",
    "    \"\"\"\n",
    "    min_split_num = 3 if 3 < max_split else max_split\n",
    "    split_num = np.random.choice(\n",
    "        range(min_split_num, max_split + 1), sample_num, replace=True\n",
    "    )\n",
    "    return list(\n",
    "        map(\n",
    "            lambda split: generate_wrapper(\n",
    "                max_length, split, noise_std, steady_angle, beta_allowed_section, steady_alpha\n",
    "            ),\n",
    "            split_num,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_size(\n",
    "    n_cols: int,\n",
    "    time_col: str,\n",
    "    key_cols: List[str],\n",
    "    class_col: str,\n",
    "):\n",
    "    temp_float = 0.0\n",
    "    temp_object = \"temp\"\n",
    "    \n",
    "    # make one row data frame\n",
    "    one_row_dict = {}\n",
    "    one_row_dict.update({time_col: [temp_object]})\n",
    "    one_row_dict.update({key_col: [temp_object] for key_col in key_cols})\n",
    "    one_row_dict.update({class_col: [temp_object]})\n",
    "    one_row_dict.update({f\"column_{i}\": [temp_object] for i in range(n_cols)})\n",
    "    one_row_df = pd.DataFrame(one_row_dict)\n",
    "    \n",
    "    # caculate one row data frame's file size in csv\n",
    "    temp_path = \"./temp_row_files.csv\"\n",
    "    one_row_df.to_csv(temp_path, header=None)\n",
    "    file_size = os.stat(temp_path).st_size / (1024 * 1024)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    return file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timestamp_list(length, interval_seconds=1):\n",
    "    start_time = pd.to_datetime(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    end_time = start_time + pd.DateOffset(seconds=length * interval_seconds - 1)\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, freq=str(interval_seconds) + 'S')\n",
    "    return timestamps.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_df(\n",
    "    n_sample_rows: int,\n",
    "    n_cols: int,\n",
    "    time_col: str,\n",
    "    key_cols: List[str],\n",
    "    class_col: str,\n",
    "    label: int,\n",
    "    sample_idx: int,\n",
    "    sample_value_list: List[List[float]]\n",
    "):\n",
    "    sample_dict = {}\n",
    "    sample_dict.update({time_col: generate_timestamp_list(n_sample_rows)})\n",
    "    sample_dict.update({key_col_name: [f\"sample_{sample_idx}\"]*n_sample_rows for key_col_name in key_cols})\n",
    "    sample_dict.update({class_col: [label]*n_sample_rows})\n",
    "    sample_dict.update({f\"column_{i}\": sample_value_list[i] for i in range(n_cols)})\n",
    "    sample_df = pd.DataFrame(sample_dict)\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(label_ratio, n_samples):\n",
    "    label_arr = np.zeros(n_samples).astype(int)\n",
    "    n_normal = int(n_samples*label_ratio)\n",
    "    label_arr[:n_normal] = 1\n",
    "    np.random.shuffle(label_arr)\n",
    "    label_list = list(label_arr)\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_value_list(n_cols, n_sample_rows):\n",
    "    return [generate_sample(n_sample_rows, 5, 1)[0][1] for _ in range(n_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pr_sample(\n",
    "    file_size: float,\n",
    "    n_cols: int,\n",
    "    n_samples: int,\n",
    "    time_col: str,\n",
    "    key_cols: List[str],\n",
    "    class_col: str,\n",
    "    label_ratio: float=0.5,\n",
    "    save=False,\n",
    "    save_path: str=\"sample.csv\",\n",
    "    print_description=True\n",
    "):\n",
    "\n",
    "    row_size = get_row_size(n_cols, time_col, key_cols, class_col)\n",
    "    n_rows = int(np.round(file_size/row_size))\n",
    "    n_sample_rows = int(np.round(n_rows/n_samples))\n",
    "    \n",
    "    label_list = get_label_list(label_ratio, n_samples)\n",
    "    sensor_value_list = get_sensor_value_list(n_cols, n_sample_rows)\n",
    "    sample_df_list = []\n",
    "    for sample_idx in range(n_samples):\n",
    "        sample_label = label_list[sample_idx]\n",
    "        sample_df_list.append(get_sample_df(n_sample_rows, n_cols, time_col, key_cols, class_col, sample_label, sample_idx, sensor_value_list))\n",
    "    concat_df = pd.concat(sample_df_list).reset_index(drop=True)\n",
    "    \n",
    "    if save:\n",
    "        concat_df.to_csv(save_path, index=False)\n",
    "        \n",
    "    if print_description:\n",
    "        None\n",
    "    \n",
    "    concat_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = 0.4\n",
    "n_cols = 10\n",
    "n_samples = 100\n",
    "time_col = \"time\"\n",
    "key_cols = [\"lotid\", \"recipe\", \"wafer_id\"]\n",
    "class_col = \"leak\"\n",
    "label_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = generate_pr_sample(\n",
    "    file_size,\n",
    "    n_cols,\n",
    "    n_samples,\n",
    "    time_col,\n",
    "    key_cols,\n",
    "    class_col,\n",
    "    label_ratio,\n",
    "    save=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apollo_studio_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
